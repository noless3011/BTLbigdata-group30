# BTLbigdata-group30

Há»‡ thá»‘ng thu tháº­p, lÆ°u trá»¯, phÃ¢n tÃ­ch vÃ  xá»­ lÃ½ káº¿t quáº£ há»c táº­p cá»§a sinh viÃªn Ä‘á»ƒ dá»± Ä‘oÃ¡n Ä‘iá»ƒm sá»‘

## ðŸ“‹ Tuáº§n 5 - PhÃ¢n chia cÃ´ng viá»‡c

### Ingestion Layer

- **Kafka streaming**: Thá»‹nh, PhÃº, Tiáº¿n
- **Batch ingestion to HDFS**: LÃ¢m, Lá»™c

**Má»¥c tiÃªu**: Trong 1 tuáº§n pháº£i xong ingestion layer

---

## ðŸ“š Kafka Learning Resources

### For Streaming Team (Thá»‹nh, PhÃº, Tiáº¿n)

**Start Here**: [`kafka/README.md`](kafka/README.md)

**Learning Path** (1 week):

1. **Day 1-2**: Understand Kafka basics (`kafka/README.md` sections 1-2)
2. **Day 3-4**: Complete tutorials (`kafka/01-basic-producer-consumer/`, `kafka/02-json-messages/`)
3. **Day 5-6**: Implement project examples (`kafka/project-examples/`)
4. **Day 7**: Integration testing & documentation

**Key Files**:

- ðŸ“– `kafka/README.md` - Complete learning guide
- ðŸŽ¯ `kafka/01-basic-producer-consumer/` - Your first Kafka app
- ðŸ“Š `kafka/02-json-messages/` - Working with structured data
- ðŸš€ `kafka/project-examples/` - Production-ready code for our project

**What You'll Build**:

- Student activity producer (send events to Kafka)
- Spark Structured Streaming consumer (process events in real-time)
- Integration with MongoDB (store processed data)

---

## ðŸ—‚ï¸ Project Structure

```
BTLbigdata-group30/
â”œâ”€â”€ kafka/                          # Kafka Kubernetes configs
â”‚   â”œâ”€â”€ deployment.yaml             # Kafka cluster (3 nodes)
â”‚   â”œâ”€â”€ topics.yaml                 # Topic definitions (6 topics)
â”‚   â”œâ”€â”€ storage-class.yaml          # Storage configuration
â”‚   â”œâ”€â”€ persistent-volumn.yaml      # Multi-node PVs (production)
â”‚   â””â”€â”€ persistent-volumn-minikube.yaml  # Single-node PVs (testing)
â”œâ”€â”€ minio/                          # MinIO deployment
â”‚   â””â”€â”€ deployment.yaml             # S3-compatible storage
â”œâ”€â”€ spark/                          # Spark jobs
â”‚   â””â”€â”€ ingestion-job.yaml          # Ingestion K8s job
â”œâ”€â”€ docs/                           # Documentation
â”‚   â””â”€â”€ event-schema-specification.md  # Event schemas
â”œâ”€â”€ producer.py                     # Event generator (6 categories)
â”œâ”€â”€ ingest_layer.py                # Kafka â†’ HDFS ingestion
â”œâ”€â”€ minio_ingest_k8s.py            # Kafka â†’ MinIO ingestion (K8s)
â”œâ”€â”€ docker-compose.yml              # Local development setup
â”œâ”€â”€ deploy_minikube.ps1            # Auto-deploy to Minikube
â”œâ”€â”€ cleanup_minikube.ps1           # Cleanup Minikube
â”œâ”€â”€ MINIKUBE_TESTING_GUIDE.md      # Complete testing guide
â”œâ”€â”€ MINIKUBE_QUICK_REFERENCE.md    # Command reference
â”œâ”€â”€ TESTING_COMPARISON.md          # Local vs Minikube comparison
â”œâ”€â”€ 01-basic-producer-consumer/ # Tutorial 1
â”‚   â”œâ”€â”€ 02-json-messages/           # Tutorial 2
â”‚   â”œâ”€â”€ 03-partitions/              # Tutorial 3 (coming soon)
â”‚   â”œâ”€â”€ 04-consumer-groups/         # Tutorial 4 (coming soon)
â”‚   â””â”€â”€ project-examples/           # Production code
â”‚       â”œâ”€â”€ student_activity_producer.py
â”‚       â”œâ”€â”€ attendance_producer.py
â”‚       â””â”€â”€ spark_streaming_consumer.py
â”‚
â”œâ”€â”€ generate_fake_data/             # Data generation (existing)
â”œâ”€â”€ problem-definition.md           # Project requirements
â”œâ”€â”€ architecture-design.md          # System architecture
â”œâ”€â”€ deployment-guide.md             # Setup instructions
â””â”€â”€ docker-compose.yml              # Local development environment
```

---

## ðŸ§ª Testing on Minikube (Kubernetes)

### Quick Start for Minikube Testing

**Prerequisites**: Install Minikube and kubectl

```powershell
choco install minikube kubernetes-cli
```

**Deploy Everything**:

```powershell
.\deploy_minikube.ps1
```

**Test Ingestion**:

1. Open 3 terminals for port forwarding:
   ```powershell
   # Terminal 1: Kafka
   kubectl port-forward service/kafka-cluster-kafka-bootstrap 9092:9092 -n kafka
   
   # Terminal 2: MinIO API
   kubectl port-forward service/minio 9000:9000 -n minio
   
   # Terminal 3: MinIO Console
   kubectl port-forward service/minio 9001:9001 -n minio
   ```

2. Run producer (Terminal 4):
   ```powershell
   python producer.py
   ```

3. Run ingestion (Terminal 5):
   ```powershell
   python minio_ingest_k8s.py
   ```

4. Verify data in MinIO Console: http://localhost:9001 (minioadmin/minioadmin)

**Cleanup**:

```powershell
.\cleanup_minikube.ps1
```

ðŸ“š **Detailed Guide**: [MINIKUBE_TESTING_GUIDE.md](MINIKUBE_TESTING_GUIDE.md)  
âš¡ **Quick Reference**: [MINIKUBE_QUICK_REFERENCE.md](MINIKUBE_QUICK_REFERENCE.md)

---

## ðŸš€ Quick Start for Kafka Team

---

## ðŸ›  Installation & Setup Instructions

Follow these steps strictly in order to set up the Lambda Architecture environment.

### 1. Prerequisites

- **Python 3.8 - 3.11** (Avoid 3.12+ for now due to PySpark compatibility).
- **Docker Desktop** (Running).
- **Amazon Corretto JDK 8 or 11** installed (Ensure `JAVA_HOME` is set automatically by the installer).

### 2. Python Environment Setup

Create a virtual environment to keep dependencies isolated.

**Windows:**

```bash
python -m venv venv
.\venv\Scripts\activate
```

**Mac/Linux:**

```bash
python3 -m venv venv
source venv/bin/activate
```

**Install Dependencies:**

```bash
pip install -r requirements.txt
```

### 3. Network Configuration (Crucial)

You must map the Docker container hostnames to your local machine so PySpark can talk to HDFS and Kafka.

**Windows:**
Open Notepad as Administrator and edit: `C:\Windows\System32\drivers\etc\hosts`

**Mac/Linux:**
Open Terminal and edit: `sudo nano /etc/hosts`

**Add this line to the bottom of the file:**

```text
127.0.0.1 namenode datanode kafka
```

### 4. Start Infrastructure (Docker)

Spin up Zookeeper, Kafka, and Hadoop (HDFS).

```bash
docker-compose up -d
```

> â³ **Wait 1-2 minutes** after this command for the NameNode and DataNode to fully initialize (Safemode OFF).

---

## ðŸš€ Execution Guide (Pipeline Order)

Open multiple terminal tabs/windows to run the components of the Lambda Architecture.

#### Terminal 1: Data Source (Simulation)

Start generating fake events to Kafka. Keep this running.

```bash
python producer.py
```

#### Terminal 2: Speed Layer (Real-time)

Process data directly from Kafka streams.

```bash
python stream_layer.py
```

#### Terminal 3: Ingest Layer (Data Lake)

Capture data from Kafka and save it to HDFS (Simulating "Immutable Master Data").

```bash
python ingest_layer.py
```

> _Note: Let this run for 10-20 seconds to capture enough data, then stop it (Ctrl+C). It should automatically upload the data to HDFS._

#### Terminal 4: Batch Layer (Historical Processing)

Once data is on HDFS (from the previous step), run the batch job to create pre-computed views (Parquet files).

```bash
python batch_layer.py
```

#### Terminal 5: Serving Layer (Query)

Query the final unified view (merging Batch Views + Real-time Views).

```bash
python serving_layer.py
```

---

### ðŸ§¹ Cleanup

To stop the containers and free up resources:

```bash
docker-compose down
```

---

## ðŸ“ž Support

- **Questions about Kafka?** â†’ Check `kafka/README.md` or ask in team chat
- **Stuck on a tutorial?** â†’ Review the code comments (detailed explanations)
- **Need help?** â†’ Contact team leads

---

**Next Milestone**: Ingestion layer complete by end of Week 5! ðŸŽ¯
