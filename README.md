# BTLbigdata-group30

Há»‡ thá»‘ng thu tháº­p, lÆ°u trá»¯, phÃ¢n tÃ­ch vÃ  xá»­ lÃ½ káº¿t quáº£ há»c táº­p cá»§a sinh viÃªn Ä‘á»ƒ dá»± Ä‘oÃ¡n Ä‘iá»ƒm sá»‘

## ðŸ“‹ Tuáº§n 5 - PhÃ¢n chia cÃ´ng viá»‡c

### Ingestion Layer

- **Kafka streaming**: Thá»‹nh, PhÃº, Tiáº¿n
- **Batch ingestion to HDFS**: LÃ¢m, Lá»™c

**Má»¥c tiÃªu**: Trong 1 tuáº§n pháº£i xong ingestion layer

---

## ðŸ“š Kafka Learning Resources

### For Streaming Team (Thá»‹nh, PhÃº, Tiáº¿n)

**Start Here**: [`kafka/README.md`](kafka/README.md)

**Learning Path** (1 week):

1. **Day 1-2**: Understand Kafka basics (`kafka/README.md` sections 1-2)
2. **Day 3-4**: Complete tutorials (`kafka/01-basic-producer-consumer/`, `kafka/02-json-messages/`)
3. **Day 5-6**: Implement project examples (`kafka/project-examples/`)
4. **Day 7**: Integration testing & documentation

**Key Files**:

- ðŸ“– `kafka/README.md` - Complete learning guide
- ðŸŽ¯ `kafka/01-basic-producer-consumer/` - Your first Kafka app
- ðŸ“Š `kafka/02-json-messages/` - Working with structured data
- ðŸš€ `kafka/project-examples/` - Production-ready code for our project

**What You'll Build**:

- Student activity producer (send events to Kafka)
- Spark Structured Streaming consumer (process events in real-time)
- Integration with MongoDB (store processed data)

---

## ðŸ—‚ï¸ Project Structure

```
BTLbigdata-group30/
â”œâ”€â”€ kafka/                          # Kafka learning & examples (NEW!)
â”‚   â”œâ”€â”€ README.md                   # Complete Kafka guide
â”‚   â”œâ”€â”€ 01-basic-producer-consumer/ # Tutorial 1
â”‚   â”œâ”€â”€ 02-json-messages/           # Tutorial 2
â”‚   â”œâ”€â”€ 03-partitions/              # Tutorial 3 (coming soon)
â”‚   â”œâ”€â”€ 04-consumer-groups/         # Tutorial 4 (coming soon)
â”‚   â””â”€â”€ project-examples/           # Production code
â”‚       â”œâ”€â”€ student_activity_producer.py
â”‚       â”œâ”€â”€ attendance_producer.py
â”‚       â””â”€â”€ spark_streaming_consumer.py
â”‚
â”œâ”€â”€ generate_fake_data/             # Data generation (existing)
â”œâ”€â”€ problem-definition.md           # Project requirements
â”œâ”€â”€ architecture-design.md          # System architecture
â”œâ”€â”€ deployment-guide.md             # Setup instructions
â””â”€â”€ docker-compose.yml              # Local development environment
```

---

## ðŸš€ Quick Start for Kafka Team

---

## ðŸ›  Installation & Setup Instructions

Follow these steps strictly in order to set up the Lambda Architecture environment.

### 1. Prerequisites

- **Python 3.8 - 3.11** (Avoid 3.12+ for now due to PySpark compatibility).
- **Docker Desktop** (Running).
- **Amazon Corretto JDK 8 or 11** installed (Ensure `JAVA_HOME` is set automatically by the installer).

### 2. Python Environment Setup

Create a virtual environment to keep dependencies isolated.

**Windows:**

```bash
python -m venv venv
.\venv\Scripts\activate
```

**Mac/Linux:**

```bash
python3 -m venv venv
source venv/bin/activate
```

**Install Dependencies:**

```bash
pip install -r requirements.txt
```

### 3. Network Configuration (Crucial)

You must map the Docker container hostnames to your local machine so PySpark can talk to HDFS and Kafka.

**Windows:**
Open Notepad as Administrator and edit: `C:\Windows\System32\drivers\etc\hosts`

**Mac/Linux:**
Open Terminal and edit: `sudo nano /etc/hosts`

**Add this line to the bottom of the file:**

```text
127.0.0.1 namenode datanode kafka
```

### 4. Start Infrastructure (Docker)

Spin up Zookeeper, Kafka, and Hadoop (HDFS).

```bash
docker-compose up -d
```

> â³ **Wait 1-2 minutes** after this command for the NameNode and DataNode to fully initialize (Safemode OFF).

---

## ðŸš€ Execution Guide (Pipeline Order)

Open multiple terminal tabs/windows to run the components of the Lambda Architecture.

#### Terminal 1: Data Source (Simulation)

Start generating fake events to Kafka. Keep this running.

```bash
python producer.py
```

#### Terminal 2: Speed Layer (Real-time)

Process data directly from Kafka streams.

```bash
python stream_layer.py
```

#### Terminal 3: Ingest Layer (Data Lake)

Capture data from Kafka and save it to HDFS (Simulating "Immutable Master Data").

```bash
python ingest_layer.py
```

> _Note: Let this run for 10-20 seconds to capture enough data, then stop it (Ctrl+C). It should automatically upload the data to HDFS._

#### Terminal 4: Batch Layer (Historical Processing)

Once data is on HDFS (from the previous step), run the batch job to create pre-computed views (Parquet files).

```bash
python batch_layer.py
```

#### Terminal 5: Serving Layer (Query)

Query the final unified view (merging Batch Views + Real-time Views).

```bash
python serving_layer.py
```

---

### ðŸ§¹ Cleanup

To stop the containers and free up resources:

```bash
docker-compose down
```

---

## ðŸ“ž Support

- **Questions about Kafka?** â†’ Check `kafka/README.md` or ask in team chat
- **Stuck on a tutorial?** â†’ Review the code comments (detailed explanations)
- **Need help?** â†’ Contact team leads

---

**Next Milestone**: Ingestion layer complete by end of Week 5! ðŸŽ¯
