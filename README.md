# BTLbigdata-group30 - University Learning Analytics System

Lambda Architecture implementation for student learning analytics with Kafka, Spark, MinIO, Cassandra, Trino, and Airflow.

---

## ğŸ“ Project Structure (Reorganized)

```
BTLbigdata-group30/
â”œâ”€â”€ airflow/                        # Airflow Orchestration
â”‚   â”œâ”€â”€ dags/                      # Application DAGs
â”‚   â””â”€â”€ kubernetes/                # Airflow Deployment
â”‚
â”œâ”€â”€ cassandra/                      # Cassandra Batch Views
â”‚   â”œâ”€â”€ deployment.yaml            # Deployment
â”‚   â””â”€â”€ schema.cql                 # Database Schema
â”‚
â”œâ”€â”€ trino/                          # Trino Query Engine
â”‚   â”œâ”€â”€ deployment.yaml            # Deployment
â”‚   â””â”€â”€ catalog/                   # Data Catalogs (Cassandra, MinIO)
â”‚
â”œâ”€â”€ batch_layer/                    # Batch Processing Layer âœ…
â”‚   â”œâ”€â”€ jobs/                      # PySpark batch jobs (5 jobs)
â”‚   â”œâ”€â”€ Dockerfile                 # Batch layer image
â”‚   â”œâ”€â”€ run_batch_jobs.py          # Job Runner
â”‚   â””â”€â”€ jobs/spark_config.py       # Spark & Cassandra Config
â”‚
â”œâ”€â”€ ingestion_layer/                # Kafka â†’ Storage Ingestion
â”‚   â””â”€â”€ ...                        # (Unchanged)
â”‚
â”œâ”€â”€ speed_layer/                    # Real-Time Stream Processing â³
â”‚   â””â”€â”€ ...                        # (Unchanged)
â”‚
â”œâ”€â”€ serving_layer/                  # Unified Query Interface â³
â”‚   â””â”€â”€ ...                        # (Unchanged)
â”‚
â”œâ”€â”€ deployment/                     # Deployment scripts & guides
â”‚   â”œâ”€â”€ deploy_minikube.sh        # Auto-deploy EVERYTHING
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ README.md                       # This file
```

---

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DATA SOURCES                                â”‚
â”‚            (Student Learning Events - 6 Categories)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   INGESTION LAYER (Kafka)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“                                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   BATCH LAYER âœ…       â”‚        â”‚   SPEED LAYER â³              â”‚
â”‚   (Historical Data)    â”‚        â”‚   (Real-Time Data)            â”‚
â”‚                        â”‚        â”‚                               â”‚
â”‚ â€¢ Airflow Orchestrator â”‚        â”‚ â€¢ Spark Streaming             â”‚
â”‚ â€¢ PySpark Jobs         â”‚        â”‚ â€¢ Windowed Aggregations       â”‚
â”‚ â€¢ Cassandra Storage    â”‚        â”‚ â€¢ Incremental Updates         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚     SERVING LAYER (Trino) â³     â”‚
         â”‚  (Unified Query Interface)       â”‚
         â”‚                                  â”‚
         â”‚  Query Batch Views (Cassandra)   â”‚
         â”‚  Query Data Lake (MinIO)         â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start (Minikube)

### 1. Deploy Infrastructure
Run the unified deployment script to set up Kafka, MinIO, Cassandra, Trino, and Airflow.

```bash
cd deployment
./deploy_minikube.sh
```

**Note**: This requires significant resources (allocates 6 CPUs, 12GB RAM to Minikube).

### 2. Access Services
The script will output port-forwarding commands. Open separate terminals:

```bash
# MinIO Console
kubectl port-forward service/minio 9001:9001 -n minio

# Airflow UI (admin/admin)
kubectl port-forward service/airflow 8080:8080 -n default

# Trino UI
kubectl port-forward service/trino 8080:8080 -n default
```

- **Airflow**: [http://localhost:8080](http://localhost:8080)
- **MinIO**: [http://localhost:9001](http://localhost:9001)

### 3. Initialize Database
Connect to Cassandra to create the schema:

```bash
kubectl exec -it deployment/cassandra -- cqlsh
```
*Tip: Copy contents of `cassandra/schema.cql` and paste into `cqlsh` prompt.*

### 4. Run Data Pipeline

1.  **Generate Events**:
    ```bash
    python ingestion_layer/producer.py
    ```
2.  **Ingest to MinIO**:
    ```bash
    python ingestion_layer/minio_ingest_k8s.py
    ```
3.  **Trigger Batch Jobs (Airflow)**:
    - Go to Airflow UI.
    - Unpause the `batch_layer_pipeline` DAG.
    - Trigger the DAG manually to run all batch jobs.

### 5. Query Results (Trino)

Connect to Trino using CLI or DBeaver (JDBC):

```sql
-- Query Authentication Stats from Cassandra
SELECT * FROM cassandra.lms_analytics.auth_daily_active_users;

-- Query Video Engagement
SELECT * FROM cassandra.lms_analytics.video_popularity ORDER BY total_views_seconds DESC;
```

---

## ğŸ”§ Batch Layer Details

The batch layer has been upgraded to write to **Cassandra** for low-latency serving.

- **Source**: MinIO (Raw Events, Parquet)
- **Processing**: PySpark (Orchestrated by Airflow)
- **Sink**:
    1.  **Cassandra** (Primary Serving Store)
    2.  **MinIO** (Parquet Backup - `batch_views/`)

### Airflow DAG
The `batch_layer_pipeline` DAG runs daily and executes all 5 batch jobs in parallel using `KubernetesPodOperator`.

### Spark-Cassandra Integration
Jobs use `spark-cassandra-connector` to write DataFrames directly to Cassandra tables.

---

## ğŸ¯ Status

| Component | Status | Tech Stack |
|:---|:---|:---|
| **Ingestion** | âœ… Complete | Kafka, MinIO |
| **Batch Processing** | âœ… Complete | Spark, Airflow |
| **Batch Storage** | âœ… Complete | Cassandra, MinIO |
| **Orchestration** | âœ… Complete | Airflow |
| **Serving Query** | âœ… Complete | Trino |
| **Speed Layer** | â³ In Progress | Spark Streaming |

---

## ğŸ‘¥ Team - Group 30
